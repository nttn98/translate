<!DOCTYPE html>
<html lang="vi">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vietnamese to English Translator</title>
    <link rel="stylesheet" href="/static/style.css" />
  </head>
  <body>
    <div class="container">
      <h1>Vietnamese to English Translator</h1>
      <div class="status-bar" id="status">Ready</div>

      <div class="section">
        <div class="section-header">
          <span class="section-title">Vietnamese Input</span>
          <div style="display: flex; gap: 8px">
            <button
              class="btn btn-primary"
              id="voiceBtn"
              onclick="handleVoiceInput()"
            >
              üé§ Voice Input
            </button>
            <!-- Fallback record button (MediaRecorder) -->
            <button
              class="btn btn-primary"
              id="recordBtn"
              onclick="toggleRecording()"
            >
              ‚è∫Ô∏è Record
            </button>
          </div>
        </div>
        <textarea
          id="inputText"
          placeholder="Type Vietnamese here..."
        ></textarea>
        <div class="note">
          Tip: M·ªü trang b·∫±ng <strong>http://localhost</strong> ho·∫∑c
          <strong>https</strong> ƒë·ªÉ voice ho·∫°t ƒë·ªông. N·∫øu g·∫∑p l·ªói "network", th·ª≠
          t·∫Øt VPN ho·∫∑c ƒë·ªïi m·∫°ng. N·∫øu Web Speech l·ªói, d√πng "Record" (upload file
          cho server-side STT).
        </div>
      </div>

      <div class="output-section">
        <div class="section-header">
          <span class="section-title">English Translation</span>
        </div>
        <div class="output-text" id="outputText">
          Translation will appear here...
        </div>
        <div class="button-group">
          <button class="btn btn-success" id="speakBtn" onclick="handleSpeak()">
            üîä Speak
          </button>
          <button class="btn btn-danger" onclick="handleStop()">‚èπÔ∏è Stop</button>
        </div>
        <div class="small">
          Dev logs: open DevTools ‚Üí Console ƒë·ªÉ xem chi ti·∫øt speech logs.
        </div>
      </div>

      <div class="settings">
        <h3>Settings</h3>
        <div class="setting-item">
          <label>Speech Speed: <span id="speedValue">1.0x</span></label>
          <input
            type="range"
            id="speedSlider"
            min="0.5"
            max="2"
            step="0.1"
            value="1.0"
            oninput="updateSpeed(this.value)"
          />
        </div>

        <label class="checkbox-label">
          <input type="checkbox" id="autoSpeak" checked />
          Auto-speak after translation
        </label>

        <label class="checkbox-label">
          <input type="checkbox" id="continuous" checked />
          Continuous mode (only new text)
        </label>
      </div>

      <div class="features">
        <strong>‚ú® Features:</strong>
        <ul>
          <li>Auto-translates 1 second after typing</li>
          <li>Auto-speak with continuous mode (only reads new content)</li>
          <li>
            Voice input support for Vietnamese (Web Speech API) with robust
            error handling
          </li>
          <li>Record fallback ‚Äî uploads to server-side STT (/stt)</li>
        </ul>
      </div>
    </div>

    <script>
      /* -------------------------
         State (typing/TTS untouched)
         ------------------------- */
      let typingTimer;
      let lastText = "";
      let lastPosition = 0;
      let isSpeaking = false;
      let isListening = false;

      // --- Voice (Web Speech) vars ---
      let recognition = null;
      let voiceRequested = false; // whether user asked to keep listening (used to allow single auto-restart)

      // --- Recording fallback vars ---
      let mediaRecorder = null;
      let recordedChunks = [];
      let isRecording = false;

      // Utility
      function updateStatus(s) {
        const el = document.getElementById("status");
        if (el) el.textContent = s;
        console.log("[STATUS]", s);
      }

      function updateSpeed(v) {
        document.getElementById("speedValue").textContent =
          parseFloat(v).toFixed(1) + "x";
      }

      function isSecureContextForSpeech() {
        return (
          location.protocol === "https:" ||
          location.hostname === "localhost" ||
          location.hostname === "127.0.0.1"
        );
      }

      /* -------------------------
         Web Speech: safe init + handlers
         Replace existing fragile logic with this version:
         - continuous = false (single-utterance) to avoid long network streams
         - no infinite auto-restart
         - clear handling of permission/network errors
         ------------------------- */
      function initRecognitionSafe() {
        if (
          !(
            "webkitSpeechRecognition" in window || "SpeechRecognition" in window
          )
        ) {
          console.warn("SpeechRecognition not supported.");
          recognition = null;
          return;
        }
        const SpeechRecognition =
          window.SpeechRecognition || window.webkitSpeechRecognition;
        try {
          recognition = new SpeechRecognition();
        } catch (err) {
          console.error("Failed to construct SpeechRecognition:", err);
          recognition = null;
          return;
        }

        recognition.lang = "vi-VN";
        recognition.continuous = false; // single utterance ‚Äî safer
        recognition.interimResults = false;
        recognition.maxAlternatives = 1;

        recognition.onstart = () => {
          isListening = true;
          updateStatus("Listening...");
          console.log("recognition.onstart");
        };

        recognition.onresult = (event) => {
          let transcript = "";
          for (let i = event.resultIndex; i < event.results.length; ++i) {
            transcript += event.results[i][0].transcript;
          }
          console.log("recognition.onresult:", transcript);
          // Preserve typing behavior: dispatch 'input' to reuse your typing debounce/autotranslate
          const inputEl = document.getElementById("inputText");
          if (inputEl) {
            inputEl.value = transcript;
            inputEl.dispatchEvent(new Event("input", { bubbles: true }));
          } else {
            // fallback: call translateText directly if input missing
            if (typeof translateText === "function") translateText(transcript);
          }
        };

        recognition.onerror = (e) => {
          const err = e && e.error ? e.error : "unknown";
          console.error("recognition.onerror:", e);
          updateStatus("Error: " + err);
          isListening = false;
          voiceRequested = false;
          // reset UI
          const vb = document.getElementById("voiceBtn");
          if (vb) {
            vb.className = "btn btn-primary";
            vb.textContent = "üé§ Voice Input";
          }

          if (err === "network") {
            // network error => likely Web Speech backend unreachable
            alert(
              "Speech service network error. Th·ª≠: t·∫Øt VPN/proxy, ƒë·ªïi m·∫°ng, d√πng Incognito. Ho·∫∑c d√πng n√∫t Record ƒë·ªÉ upload audio cho server-side transcription."
            );
            // enable record button UI hint (no auto-record)
            console.warn(
              "Web Speech network error (suggest using Record fallback)."
            );
          } else if (err === "not-allowed" || err === "service-not-allowed") {
            alert(
              "Quy·ªÅn microphone b·ªã ch·∫∑n. H√£y b·∫≠t Microphone cho trang n√†y trong settings tr√¨nh duy·ªát."
            );
          } else {
            console.warn("SpeechRecognition error:", err);
          }
        };

        recognition.onend = () => {
          console.log(
            "recognition.onend ‚Äî isListening:",
            isListening,
            "voiceRequested:",
            voiceRequested
          );
          isListening = false;
          updateStatus("Ready");
          const vb = document.getElementById("voiceBtn");
          if (vb) {
            vb.className = "btn btn-primary";
            vb.textContent = "üé§ Voice Input";
          }

          // If user explicitly requested continued listening, restart once only
          if (voiceRequested) {
            voiceRequested = false; // allow single auto-restart
            setTimeout(() => {
              try {
                recognition.start();
                console.log("Auto-restart recognition (single attempt).");
              } catch (err) {
                console.warn("Auto-restart failed:", err);
              }
            }, 250);
          }
        };

        console.log("initRecognitionSafe done");
      }

      // initialize on load
      initRecognitionSafe();

      // Voice button handler (start/stop)
      function handleVoiceInput() {
        if (!isSecureContextForSpeech()) {
          alert(
            "SpeechRecognition requires HTTPS or http://localhost. M·ªü trang b·∫±ng https ho·∫∑c localhost."
          );
          return;
        }
        if (!recognition) {
          alert("SpeechRecognition not supported (use Chrome/Edge).");
          return;
        }

        const vb = document.getElementById("voiceBtn");
        if (isListening) {
          // stop
          try {
            recognition.stop();
          } catch (err) {
            console.warn("stop error:", err);
          }
          voiceRequested = false;
          // UI will be reset in onend
          return;
        }

        // request mic permission first (shows browser prompt)
        navigator.mediaDevices
          .getUserMedia({ audio: true })
          .then((stream) => {
            // stop immediately; we only needed permission
            stream.getTracks().forEach((t) => t.stop());
            // update UI
            if (vb) {
              vb.className = "btn btn-listening";
              vb.textContent = "üé§ Listening...";
            }
            voiceRequested = true;
            try {
              if (!isListening) recognition.start();
            } catch (err) {
              console.error("recognition.start() error:", err);
              // attempt recreate then start once
              try {
                initRecognitionSafe();
                recognition.start();
              } catch (err2) {
                console.error("Retry recognition.start failed:", err2);
                alert(
                  "Kh√¥ng th·ªÉ b·∫Øt ƒë·∫ßu voice input: " + (err2.message || err2)
                );
                if (vb) {
                  vb.className = "btn btn-primary";
                  vb.textContent = "üé§ Voice Input";
                }
                updateStatus("Ready");
              }
            }
          })
          .catch((err) => {
            console.warn("getUserMedia error:", err);
            alert(
              "Kh√¥ng th·ªÉ truy c·∫≠p microphone. Ki·ªÉm tra quy·ªÅn trong tr√¨nh duy·ªát."
            );
          });
      }

      // expose to onclick attr
      window.handleVoiceInput = handleVoiceInput;

      /* -------------------------
         MediaRecorder fallback (Record button)
         - Records audio (webm) and uploads to /stt if available
         - Server-side /stt returns { transcription: "..." } when configured (see app.py)
         ------------------------- */
      function setRecordBtn(on) {
        const b = document.getElementById("recordBtn");
        if (!b) return;
        if (on) {
          b.textContent = "‚èπÔ∏è Stop Recording";
          b.className = "btn btn-listening";
        } else {
          b.textContent = "‚è∫Ô∏è Record";
          b.className = "btn btn-primary";
        }
      }

      async function toggleRecording() {
        if (isRecording) {
          stopRecording();
          return;
        }
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: true,
          });
          recordedChunks = [];
          mediaRecorder = new MediaRecorder(stream);
          mediaRecorder.ondataavailable = (e) => {
            if (e.data && e.data.size > 0) recordedChunks.push(e.data);
          };
          mediaRecorder.onstop = async () => {
            // stop tracks
            stream.getTracks().forEach((t) => t.stop());
            const blob = new Blob(recordedChunks, { type: "audio/webm" });
            // upload to /stt
            await uploadAudioBlob(blob);
          };
          mediaRecorder.start();
          isRecording = true;
          setRecordBtn(true);
          updateStatus("Recording...");
        } catch (err) {
          console.error("getUserMedia error:", err);
          alert("Kh√¥ng th·ªÉ truy c·∫≠p microphone. Ki·ªÉm tra quy·ªÅn.");
        }
      }

      function stopRecording() {
        if (mediaRecorder && isRecording) mediaRecorder.stop();
        isRecording = false;
        setRecordBtn(false);
        updateStatus("Ready");
      }

      async function uploadAudioBlob(blob) {
        updateStatus("Uploading audio...");
        try {
          const form = new FormData();
          const filename = `rec_${Date.now()}.webm`;
          form.append("file", blob, filename);
          const resp = await fetch("/stt", { method: "POST", body: form });
          if (!resp.ok) {
            const txt = await resp.text();
            console.warn("STT upload error:", txt);
            alert("STT upload failed: " + txt);
            updateStatus("Ready");
            return;
          }
          const data = await resp.json();
          if (data.transcription) {
            // put transcription into input and reuse typing logic
            const inputEl = document.getElementById("inputText");
            if (inputEl) {
              inputEl.value = data.transcription;
              inputEl.dispatchEvent(new Event("input", { bubbles: true }));
            } else {
              if (typeof translateText === "function")
                translateText(data.transcription);
            }
            updateStatus("Ready");
          } else if (data.file) {
            alert(
              "Audio uploaded to: " +
                data.file +
                "\nNo STT provider configured on server."
            );
            updateStatus("Ready");
          } else {
            alert("STT response: " + JSON.stringify(data));
            updateStatus("Ready");
          }
        } catch (err) {
          console.error("uploadAudioBlob error:", err);
          alert("Upload failed: " + (err.message || err));
          updateStatus("Ready");
        }
      }

      // expose record functions
      window.toggleRecording = toggleRecording;

      /* -------------------------
         Typing handler (kept exactly as before)
         ------------------------- */
      document.getElementById("inputText").addEventListener("input", (e) => {
        const text = e.target.value;
        clearTimeout(typingTimer);
        if (text.trim()) {
          updateStatus("Typing...");
          typingTimer = setTimeout(() => translateText(text), 1000);
        } else {
          document.getElementById("outputText").textContent =
            "Translation will appear here...";
          lastText = "";
          lastPosition = 0;
          updateStatus("Ready");
        }
      });

      /* -------------------------
         Keep translateText/applyTranslation/mockTranslate/speech as you had them
         (paste your existing functions below or keep the ones already in your template)
         ------------------------- */

      // If your original file already included these functions, keep them unchanged.
      // For safety, include minimal implementations only if not present:

      async function translateText(text) {
        if (!text.trim()) return;
        updateStatus("Translating...");
        try {
          const response = await fetch("/translate", {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ text }),
          });
          if (response.ok) {
            const data = await response.json();
            if (data && data.translation) {
              applyTranslation(data.translation);
              return;
            }
            console.warn("Server responded but no translation field.");
          } else {
            console.warn("Server returned non-ok status:", response.status);
          }
        } catch (err) {
          console.warn("Server translate failed:", err);
        }
        const fallback = mockTranslate(text);
        applyTranslation(fallback);
      }

      function applyTranslation(trans) {
        document.getElementById("outputText").textContent = trans;
        lastText = trans;
        updateStatus("Ready");
        if (document.getElementById("autoSpeak").checked)
          setTimeout(() => speakText(trans), 300);
      }

      function mockTranslate(vnText) {
        return "EN (mock): " + vnText;
      }

      function speakText(text) {
        if (!text || text === "Translation will appear here...") return;
        let textToSpeak = text;
        if (document.getElementById("continuous").checked && lastPosition > 0) {
          const previousPart = lastText.substring(0, lastPosition);
          if (text.indexOf(previousPart) === 0) {
            textToSpeak = text.substring(lastPosition).trim();
            if (!textToSpeak) return;
          }
        }
        window.speechSynthesis.cancel();
        const u = new SpeechSynthesisUtterance(textToSpeak);
        u.lang = "en-US";
        u.rate = parseFloat(document.getElementById("speedSlider").value);
        u.onstart = () => {
          isSpeaking = true;
          updateStatus("Speaking...");
          document.getElementById("speakBtn").className = "btn btn-speaking";
        };
        u.onend = () => {
          isSpeaking = false;
          updateStatus("Ready");
          document.getElementById("speakBtn").className = "btn btn-success";
          lastPosition = text.length;
        };
        u.onerror = (e) => {
          console.error("TTS error:", e);
          isSpeaking = false;
          updateStatus("Ready");
          document.getElementById("speakBtn").className = "btn btn-success";
        };
        window.speechSynthesis.speak(u);
      }

      window.handleSpeak = function () {
        if (lastText) {
          lastPosition = 0;
          speakText(lastText);
        }
      };
      window.handleStop = function () {
        window.speechSynthesis.cancel();
        isSpeaking = false;
        updateStatus("Ready");
        document.getElementById("speakBtn").className = "btn btn-success";
      };

      // Debug helpers
      window._debug = {
        get recognition() {
          return recognition;
        },
        initRecognitionSafe,
        toggleRecording,
      };
    </script>
  </body>
</html>
